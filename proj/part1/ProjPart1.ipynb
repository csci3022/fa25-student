{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f5fcfd",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"ProjPart1.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc630dcb",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e0f9b2de18190d9d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Project - Part 1: Exploring Cook County Housing\n",
    "\n",
    "## Due Date: Thursday, Nov 20th 11:59 PM MT on Gradescope\n",
    "\n",
    "## NO LATE SUBMISSIONS will be accepted - you must plan accordingly.\n",
    "\n",
    "## Collaboration Policy\n",
    "\n",
    "Data science is a collaborative activity.  However a key step in learning and retention is **creating solutions on your own.**  \n",
    "\n",
    "Below are examples of acceptable vs unacceptable use of resources and collaboration when doing the Project assignments in CSCI 3022.\n",
    "\n",
    "\n",
    "The following would be some **examples of cheating** when working on the Project in CSCI 3022.  Any of these constitute a **violation of the course's collaboration policy and will result in an F in the course and a trip to the honor council**.   \n",
    "\n",
    "\n",
    " - Consulting web pages that may have a solution to a given homework problem or one similar is cheating.  However, consulting the class notes, and web pages that explain the material taught in class but do NOT show a solution to the homework problem in question are permissible to view.  Clearly, there's a fuzzy line here between a valid use of resources and cheating. To avoid this line, one should merely consult the course notes, the course textbook, and references that contain syntax and/or formulas.\n",
    " - Copying a segment of code or math solution of three lines or more from another student from a printout, handwritten copy, or by looking at their computer screen \n",
    " - Allowing another student to copy a segment of your code or math solution of three lines or more\n",
    " - Taking a copy of another student's work (or a solution found online) and then editing that copy\n",
    " - Reading someone else’s solution to a problem on the Project before writing your own.\n",
    " - Asking someone to write all or part of a program or solution for you.\n",
    " - Asking someone else for the code necessary to fix the error for you, other than for simple syntactical errors\n",
    " \n",
    "\n",
    "\n",
    "On the other hand, the following are some **examples of things which would NOT usually be\n",
    "considered to be cheating**:\n",
    " - Working on a Project problem on your own first and then discussing with a classmate a particular part in the problem solution where you are stuck.  After clarifying any questions you should then continue to write your solution independently.\n",
    " - Asking someone (or searching online) how a particular construct in the language works.\n",
    " - Asking someone (or searching online) how to formulate a particular construct in the language.\n",
    " - Asking someone for help in finding an error in your program.  \n",
    " - Asking someone why a particular construct does not work as you expected in a given program.\n",
    "   \n",
    "\n",
    "To test whether you are truly doing your own work and retaining what you've learned you should be able to easily reproduce from scratch and explain a Project solution that was your own when asked in office hours by a TA/Instructor or on a quiz/exam.   \n",
    "\n",
    "\n",
    "If you have difficulty in formulating the general solution to a problem on your own, or\n",
    "you have difficulty in translating that general solution into a program, it is advisable to see\n",
    "your instructor or teaching assistant rather than another student as this situation can easily\n",
    "lead to a, possibly inadvertent, cheating situation.\n",
    "\n",
    "We are here to help!  Visit HW Hours and/or post questions on Piazza!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cca2812",
   "metadata": {},
   "source": [
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f7c73e",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This project explores what can be learned from an extensive housing dataset that is embedded in a dense social context in Cook County, Illinois.\n",
    "\n",
    "In project A1 (this assignment), we will guide you through some basic Exploratory Data Analysis (EDA) to understand the structure of the data. Next, you will be adding a few new features to the dataset, while cleaning the data as well in the process. \n",
    "\n",
    "In project A2 (the following assignment), you will specify and fit a linear model for the purpose of prediction. Finally, we will analyze the model's error and brainstorm ways to improve its performance.\n",
    "\n",
    "## Grading\n",
    "Grading is broken down into autograded answers and free response. \n",
    "\n",
    "For autograded answers, the results of your code are compared to provided and/or hidden tests.\n",
    "\n",
    "For free response, readers will evaluate how well you answered the question and/or fulfilled the requirements of the question.\n",
    "\n",
    "Question | Manual | Points\n",
    "----|----|----\n",
    "1a | Yes | 4\n",
    "1b | Yes | 3\n",
    "2a | No | 3\n",
    "2b | No | 2\n",
    "3a | No | 2\n",
    "3b | Yes | 2\n",
    "4a | No | 3\n",
    "4b | No | 2\n",
    "4c | No | 2\n",
    "5a | No | 2\n",
    "5b | No | 4\n",
    "5c | Yes | 4\n",
    "6a | No | 2\n",
    "6b | No | 3\n",
    "6c | No | 4\n",
    "6d | No | 2\n",
    "7a | No | 2\n",
    "7b | No | 4  \n",
    "Total |  | 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e606b71",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-62cfd21463535cac",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "import hashlib\n",
    "\n",
    "def get_hash(num):\n",
    "    \"\"\"Helper function for assessing correctness\"\"\"\n",
    "    return hashlib.md5(str(num).encode()).hexdigest()\n",
    "\n",
    "# Plot settings\n",
    "plt.rcParams['figure.figsize'] = (12, 9)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947b374d",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "# The Data\n",
    "\n",
    "The dataset consists of over 500,000 records from Cook County, Illinois, the county where Chicago is located. The dataset has 61 features in total; the 62nd is `Sale Price`, which you will predict with linear regression in the next part of this project. An explanation of each variable can be found in the included `codebook.txt` file (you can optionally open this by first clicking the `data` folder, then clicking `codebook.txt` file in the navigation pane). Some of the columns have been filtered out to ensure this assignment doesn't become overly long when dealing with data cleaning and formatting.\n",
    "\n",
    "The data are split into training and test sets with 204,792 and 68,264 observations, respectively, but we will only be working on the training set for this part of the project.\n",
    "\n",
    "Let's first extract the data from the `cook_county_data.zip`. Notice we didn't leave the `csv` files directly in the directory because they take up too much space without some prior compression. Just run the cells below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e301999",
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile('data/cook_county_data.zip') as item:\n",
    "    item.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30c236b",
   "metadata": {},
   "source": [
    "Let's load the initial data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08240913",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e8fea30adc9d489b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "initial_data = pd.read_csv(\"cook_county_train.csv\", index_col='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4926115",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9d6d509b6e854e10",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "As a good sanity check, we should at least verify that the data shape matches the description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ecf9dd",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c841a2de55691502",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# 204792 observations and 62 features in training data\n",
    "assert initial_data.shape == (204792, 62)\n",
    "# Sale Price is provided in the training data\n",
    "assert 'Sale Price' in initial_data.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddda85d",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ce9acc2f62c96e59",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "The next order of business is getting a feel for the variables in our data.  A more detailed description of each variable is included in `codebook.txt` (in the same directory as this notebook).  **You should take some time to familiarize yourself with the codebook before moving forward.**\n",
    "\n",
    "Let's take a quick look at all the current columns in our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91036094",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4e60a7a0cda5eecf",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "initial_data.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b1f122",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_data['Description'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f71f1c2",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "# Part 1: Contextualizing the Data\n",
    "\n",
    "This dataset was collected by the [Cook County Assessor's Office](https://datacatalog.cookcountyil.gov/Property-Taxation/Archive-Cook-County-Assessor-s-Residential-Sales-D/5pge-nu6u) in order to build a model to predict the monetary value of a home. You can read more about data collection in the CCAO’s [Residential Data Integrity Preliminary Report](https://gitlab.com/ccao-data-science---modeling/ccao_sf_cama_dev/-/blob/master/documentation/Preliminary%20Report%20on%20Data%20Integrity%20June%207,%202019.pdf). \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138b5c8c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "---\n",
    "## Question 1a\n",
    "\n",
    "Craft at least two questions about housing in Cook County that could be answered with this data set and provide the type of analytical tool you would use to answer it (e.g. \"I would create a ___ plot of ___ and ___\" or \"I would calculate the ___ [summary statistic] for ___ and ____\"). Be sure to reference the columns that you would use and any additional data sets you would need to answer that question.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732cccc0",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c767f11",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "---\n",
    "## Question 1b\n",
    "\n",
    "Suppose now, in addition to the information already contained in the dataset, you also have access to several new columns containing demographic data about the owner, including race/ethnicity, gender, age, annual income, and occupation. Provide one new question about housing in Cook County that can be answered using at least one column of demographic data and at least one column of existing data and provide the type of analytical tool you would use to answer it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b412f02e",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84b857b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ba0f6926b0dafefb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br/><br/>\n",
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "# Part 2: Exploratory Data Analysis\n",
    "\n",
    "In Part 2 of this project, you will be building a linear regression model that predicts sales prices using training data, but it's important to first understand how the structure of the data informs such a model. In this section, we will make a series of exploratory visualizations and feature engineering in preparation for that prediction task.\n",
    "\n",
    "Note that we will perform EDA on the **initial data**.\n",
    "\n",
    "### Sale Price\n",
    "We begin by examining the distribution of our target variable `Sale Price`. We have provided the following helper method `plot_distribution` that you can use to visualize the distribution of the `Sale Price` using both the histogram and the box plot at the same time. Run the following 2 cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ead5f0",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-15d483a695655cea",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_distribution(data, label):\n",
    "    fig, axs = plt.subplots(nrows=2)\n",
    "\n",
    "    sns.distplot(\n",
    "        data[label], \n",
    "        ax=axs[0]\n",
    "    )\n",
    "    sns.boxplot(\n",
    "        x=data[label],\n",
    "        width=0.3, \n",
    "        ax=axs[1],\n",
    "        showfliers=False,\n",
    "    )\n",
    "\n",
    "    # Align axes\n",
    "    spacer = np.max(data[label]) * 0.05\n",
    "    xmin = np.min(data[label]) - spacer\n",
    "    xmax = np.max(data[label]) + spacer\n",
    "    axs[0].set_xlim((xmin, xmax))\n",
    "    axs[1].set_xlim((xmin, xmax))\n",
    "\n",
    "    # Remove some axis text\n",
    "    axs[0].xaxis.set_visible(False)\n",
    "    axs[0].yaxis.set_visible(False)\n",
    "    axs[1].yaxis.set_visible(False)\n",
    "\n",
    "    # Put the two plots together\n",
    "    plt.subplots_adjust(hspace=0)\n",
    "    fig.suptitle(\"Distribution of \" + label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae1e443",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution(initial_data, label='Sale Price')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3f5a09",
   "metadata": {},
   "source": [
    "We can see that the data is highly skewed to the right currently, probably because of some extremely expensive houses. To get a clearer view of the distribution, run the following cell to examine a more limited range of house prices from [\\\\$0, \\\\$1,000,000]. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5b8f50-a964-491a-bfa9-ea32634dcd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_right_outliers = initial_data[initial_data['Sale Price'] <= 1000000]['Sale Price']\n",
    "plt.hist(no_right_outliers, bins=200)\n",
    "plt.xlabel(\"Sale Price\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Distribution of Sale Price\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d8d037-22d9-4a2e-886d-4fc8f3e0a8d3",
   "metadata": {},
   "source": [
    "Interestingly, we also see this spike near in the smallest bin of our `Sale Price` distribution, something that wasn't immediately evident from our initial plot. Run the following cell to see which values are the most common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acddd65a-b784-4011-9408-3387f5c8485b",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_data['Sale Price'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2e0569-8094-4832-bf10-2ceaca1de89d",
   "metadata": {},
   "source": [
    "This should be immediately concerning. While we don't know the exact reason why so many houses were reported as \\\\$1 (most likely a placeholder value of some sort), including these data points in any model would heavily skew the fit. After all, most houses are worth more than \\\\$1!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db85e6f2",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br/>\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Question 2ai\n",
    "\n",
    "Our findings with the \\\\$1 entries indicates something interesting may be happening with small valued `Sale Price`s. \n",
    "\n",
    "In the cell below, use `value_counts()` to assess the count of entries for `Sale Prices` in the range of [0, 500) (in units dollars). \n",
    "\n",
    "Then assign `q2a` to a **list** of the **top 4 most** common `Sale Price`s as integers within the range of [0, 500) in descending order of counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544da589-ae17-4db9-9fe8-c8227fb7ef88",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142e8ffd",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2ai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b339beda-d015-4bf1-9417-3c5febd34cb1",
   "metadata": {},
   "source": [
    "This exploration has indicated to us that moving forward we should probably pick a cutoff to get rid of smaller, probably unrepresentative `Sale Price`s. \n",
    "\n",
    "As a result, **moving forward you will be asked to only look at households whose price is at least \\\\$500 dollars**. While this cutoff is slightly arbitrary, this decision was made to address these unrealistically low values. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9ef9b6-2909-4a33-a804-5e09dedaba1f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br/>\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Question 2aii\n",
    "\n",
    "On the other hand, there are also the upper-bound outliers that skewed our data in the first place. \n",
    "\n",
    "Find the proportion of how many buildings sold for over \\\\$1,000,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786c8a84-1ff9-4c58-8925-ece31adb584b",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad39d7e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2aii\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4e6a6e-00f7-44ea-8e86-7b9e32f1e7a6",
   "metadata": {},
   "source": [
    "Since there's such a small percentage of buildings that have sold over \\\\$1,000,000 it would definitely be reasonable to build a model for the [\\\\$500, \\\\$1,000,000] range. \n",
    "\n",
    "For this first part of the project though, we're going to try and build a model to handle the entire range from [\\\\$500, infinity). As a result, we will need to reason with how the outliers shape the distribution of points. \n",
    "\n",
    "One way we can make our initial visualization more legible is by log-transforming our data to neutralize the impact of the right outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6afdbc3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br/>\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Question 2b\n",
    "\n",
    "As previously established, to zoom in on the visualization of most households, we will focus only on a subset of `Sale Price` for this assignment. In addition, it may be a good idea to apply log transformation to `Sale Price`. In the cell below, reassign `training_data` to a new dataframe that is the same as `initial_data` **except with the following changes**:\n",
    "\n",
    "- `training_data` should contain only households whose price is at least $500.\n",
    "- `training_data` should contain a new `Log Sale Price` column that contains the log-transformed sale prices.\n",
    "\n",
    "**You should NOT remove the original column `Sale Price` as it will be helpful for later questions.** If you accidentally remove it, just restart your kernel and run the cells again.\n",
    "\n",
    "**Note**: This also implies from now on, our target variable in the model will be the log-transformed sale prices from the column `Log Sale Price`. \n",
    "\n",
    "*To ensure that any error from this part does not propagate to later questions, there will be no hidden tests for this question.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8069ecd",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d016b056",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26707e2d",
   "metadata": {},
   "source": [
    "Let's create a new distribution plot using the log-transformed sale prices. As a sanity check, you should see that the distribution for `Log Scale Price` is much more uniform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ff0ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution(training_data, label='Log Sale Price');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04ed436",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br><br>\n",
    "\n",
    "<br/>\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Question 3a\n",
    "\n",
    "Next, we want to explore if there is any correlation between `Log Sale Price` and the total area occupied by the property. The `codebook.txt` file tells us the column `Building Square Feet` should do the trick -- it measures \"(from the exterior) the total area, in square feet, occupied by the building\".\n",
    "\n",
    "Let's also apply a log transformation to the `Building Square Feet` column.\n",
    "\n",
    "In the following cell, create a new column `Log Building Square Feet` in our `training_data` that contains the log-transformed area occupied by each property. \n",
    "\n",
    "**You should NOT remove the original `Building Square Feet` column, as it will be used for later questions**. If you accidentally remove it, just restart your kernel and run the cells again.\n",
    "\n",
    "*To ensure that any errors from this part do not propagate to later questions, there will be no hidden tests for this question.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f516222b",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "training_data['Log Building Square Feet'] = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2110c330",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ac8532",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<br/>\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "         \n",
    "## Question 3b\n",
    "\n",
    "In the visualization below, we created a `jointplot` with `Log Building Square Feet` on the x-axis, and `Log Sale Price` on the y-axis. In addition, we fit a simple linear regression line through the bivariate scatter plot in the middle.\n",
    "\n",
    "Based on the following plot, would `Log Building Square Feet` make a good candidate as one of the features for our model? Why or why not?\n",
    "\n",
    "**Hint:** To help answer this question, ask yourself: what kind of relationship does a “good” feature share with the target variable we aim to predict?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/q2p3_jointplot.png\">\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06babe99",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471f6cdd",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br/>\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Question 4a\n",
    "\n",
    "Continuing from the previous part, as you explore the dataset, you might still run into more outliers that prevent you from creating a clear visualization or capturing the trend of the majority of the houses. \n",
    "\n",
    "Write a function `remove_outliers` that removes outliers from the dataset based off a threshold value of a variable. For example, `remove_outliers(training_data, 'Building Square Feet', lower=500, upper=8000)` should return a **copy** of `training_data` with only observations that satisfy `Building Square Feet` less than 8000 (exclusive) and `Building Square Feet` greater than 500 (exclusive).\n",
    "\n",
    "**Note:** The provided tests in this notebook simply check that the `remove_outliers` function you defined does not mutate the input data inplace. They do not check that you have implemented `remove_outliers` correctly so that it works with any data, variable, lower, and upper bound.  That will be tested in hidden tests when you upload to Gradescope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac2086b",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9186ec2ca053d0aa",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def remove_outliers(data, variable, lower=-np.inf, upper=np.inf):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "      data (data frame): the table to be filtered\n",
    "      variable (string): the column with numerical outliers\n",
    "      lower (numeric): observations with values lower than this will be removed\n",
    "      upper (numeric): observations with values higher than this will be removed\n",
    "    \n",
    "    Output:\n",
    "      a data frame with outliers removed\n",
    "      \n",
    "    Note: This function should not change mutate the contents of data.\n",
    "    \"\"\"  \n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95f7368",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14558e3e-7991-40df-a2aa-946e74542b09",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br/>\n",
    "<hr style=\"border: 1px solid #fdb515;\" /> \n",
    "\n",
    "## Question 4b \n",
    "\n",
    "To give an example on how `remove_outliers` might be useful in the future, plot the distribution of the attribute `Estimate (Land)` below using `plot_distribution` with the provided code below. From `codebook.txt`, `Estimate (Land)` estimates the market value of a property’s land from the prior tax year. \n",
    "\n",
    "Indicate if the following statement is correct. Assign your answer to `q4bstatement`. \n",
    "\n",
    "“At least 95% of the properties in `training_data` have `Estimate (Land)` values less than $500,000.00”\n",
    "\n",
    "**Note:** The provided test for this question does not confirm that you have answered correctly; only that you have assigned each variable to `True` or `False`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040ca0cc-ae7b-451e-b360-1ec076ce4ae5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Run this to see the distribution!\n",
    "plot_distribution(training_data, label='Estimate (Land)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e3991d-c84b-4f93-8d53-926d21fd777d",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9186ec2ca053d0aa",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# This should be set to True or False\n",
    "q4bstatement = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27af9f2b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662a0b1a-93a3-4ab7-bd6e-cadb8bb212e9",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br/>\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Question 4c \n",
    "\n",
    "Similar to the exploration of `Sale Price` in Question 2, you may have observed that `Estimate (Land)` contains some points that heavily skew its distribution. One way to address these issues is to transform the data to a different scale, as we did with `Sale Price`. Another way is to remove outliers. \n",
    "\n",
    "In this subpart, we define an outlier as any point that is 1.5 interquartile ranges above Q3 or below Q1 (exclusive of the boundary). \n",
    "\n",
    " - Assign `IQR` to the interquartile range of `Estimate (Land)` in the initial data. \n",
    "\n",
    " - Then, assign `q4c_training_data` to `training_data` with only rows which are not considered `Estimate (Land)` outliers using `remove_outliers`.\n",
    "\n",
    "If you need a refresher on how the IQR works, you can read this [Wikipedia page](https://en.wikipedia.org/wiki/Interquartile_range).\n",
    "\n",
    "**Hint:** The `pandas` function `quantile` [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.quantile.html) will be useful in calculating the IQR and outliers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9118889-1cb8-46c8-88f7-c68ac9373f17",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "...\n",
    "\n",
    "\n",
    "\n",
    "IQR = ...\n",
    "\n",
    "\n",
    "\n",
    "q4c_training_data = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680a573c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c62ef5-619e-4d20-9500-7545d0447fed",
   "metadata": {},
   "source": [
    "Let's create a new distribution plot using the `Estimate (Land)` with outliers removed. As a sanity check for Question 4c, you should see that the distribution for `Estimate (Land)` is much less skewed with `q4c_training_data`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd4ce90-35c0-40eb-a910-03b999977ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution(q4c_training_data, label='Estimate (Land)');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa82817c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br/><br/>\n",
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "# Part 3: Feature Engineering\n",
    "\n",
    "In this section, we will walk you through a few feature engineering techniques. \n",
    "\n",
    "### Bedrooms\n",
    "\n",
    "Let's start simple by extracting the total number of bedrooms as our first feature for the model. You may notice that the `Bedrooms` column doesn't actually exist in the original `DataFrame`! Instead, it is part of the `Description` column.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "\n",
    "    \n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Question 5a\n",
    "\n",
    "Let's take a closer look at the `Description` column first. Compare the description across a few rows together at the same time. For the following list of variables, how many of them can be extracted from the `Description` column? Assign your answer to a list of integers corresponding to the statements that you think are true (ie. `[1, 2, 3]`).\n",
    "\n",
    "1. The date the property was sold on.\n",
    "2. The number of stories the property contains.\n",
    "3. The previous owner of the property.\n",
    "4. The address of the property.\n",
    "5. The number of garages the property has.\n",
    "6. The total number of rooms inside the property.\n",
    "7. The total number of bedrooms inside the property.\n",
    "8. The total number of bathrooms inside the property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b036c123",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# optional cell for scratch work "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a251e082",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "q5a = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230ebe24",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6eaa085",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "**Regex**\n",
    "\n",
    "\n",
    "A **regular expression** (“RegEx”) is a sequence of characters that specifies a search pattern. They are written to extract specific information from text. Regular expressions are essentially part of a smaller programming language embedded in python, made available through the re module. As such, they have a stand-alone syntax and methods for various capabilities.\n",
    "\n",
    "There are a ton of resources to learn and experiment with regular expressions. A few are provided below:\n",
    "\n",
    " - [Regex Reference Sheet](https://ds100.org/sp22/resources/assets/hw/regex_reference.pdf)\n",
    " - [Official Regex Guide](https://docs.python.org/3/howto/regex.html)\n",
    " - [Regex101.com](https://regex101.com/)\n",
    "  - Be sure to check Python under the category on the left.\n",
    "\n",
    "\n",
    "Below we work through an example of how to use `regex` to extract the total number of bathrooms from the `Description` column.    \n",
    "\n",
    "Then you'll apply this knowledge to write your own function to add a column to the dataset with the total number of bedrooms (**as an integer**) for each house.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf78a857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out Description column and notice where/how bathrooms are saved:\n",
    "print(training_data[\"Description\"][1])\n",
    "print(\"\")\n",
    "print(training_data[\"Description\"][60])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27181d06",
   "metadata": {},
   "source": [
    "Notice the number of bathrooms always shows up in the following statement \"and x.y of which are bathrooms.\" \n",
    "\n",
    "Wait, why are these floating point numbers?  Here's how full versus half bathrooms are defined:\n",
    " - A full bathroom consists of four main elements — a sink, toilet, shower, and bathtub. \n",
    " - A half-bath is a bathroom that contains two of the four elements, usually a sink and toilet, though in some cases a half-bath could just contain a shower and tub. \n",
    "\n",
    "\n",
    "To extract \"x.y\" as a float from the description column, we can use the following Regular Expression:\n",
    "\n",
    "\n",
    "`bathrooms_regex = r'and (\\d+\\.\\d) of which are bathrooms'`\n",
    "\n",
    "\n",
    " - The `r` preceding the regular expression pattern specifies the regular expression is a raw string. Raw strings do not recognize escape sequences (i.e., the Python newline metacharacter \\n). This makes them useful for regular expressions, which often contain literal \\ characters. In other words, don’t forget to tag your RegEx with an r.\n",
    "\n",
    "\n",
    " - The `(\\d+)` is a Regular Expression pattern:\n",
    "   -  The ( ) specifies the group that you want to capture.\n",
    " \n",
    "   - `\\d` is a regex pattern for digit (0-9)\n",
    "\n",
    "   - `+` is a regex pattern for at least (one or more) \n",
    " \n",
    "   - `\\.` is a decimal point\n",
    "   \n",
    "   - We include a second `\\d` since we want to grab the number after the decimal point (that is whether there are any 1/2 bathrooms as well)\n",
    " \n",
    "   \n",
    " \n",
    " \n",
    " We can then use [str.extract](https://pandas.pydata.org/docs/reference/api/pandas.Series.str.extract.html) to output this information:\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f2a6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the first 10 rows to use as an example set\n",
    "example_df = training_data[[\"Description\"]][:10].copy()\n",
    "\n",
    "# Create a Regex with the format of the bathroom information\n",
    "bathrooms_regex = r'and (\\d+\\.\\d) of which are bathrooms.'\n",
    "\n",
    "# Extract the bathroom information from the \"Description\" column\n",
    "example_output = example_df[\"Description\"].str.extract(bathrooms_regex).astype(float)\n",
    "\n",
    "example_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106df96b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "\n",
    "<br><br>\n",
    "\n",
    "<br/>\n",
    "<hr style=\"border: 1px solid #fdb515;\" /> \n",
    "\n",
    "## Question 5b\n",
    "\n",
    "Now it's your turn: \n",
    "\n",
    "Write a function `add_total_bedrooms(data)` that returns a copy of `data` with an additional column called `Bedrooms` that contains the **total number of bedrooms (as integers**) for each house. Treat missing values as zeros, if necessary. Remember that you can make use of vectorized code here; you shouldn't need any `for` statements. \n",
    "\n",
    "**Hint**: You should consider inspecting the `Description` column to figure out if there is any general structure within the text. Once you have noticed a certain pattern, you are set with the power of Regex!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f847a012",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def add_total_bedrooms(data):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "      data (data frame): a data frame containing at least the Description column.\n",
    "    Output:\n",
    "      a Dataframe with a new column \"Bedrooms\" containing ints.\n",
    "    \n",
    "    \"\"\"\n",
    "    with_rooms = data.copy()\n",
    "\n",
    "    ...\n",
    "    \n",
    "    return with_rooms\n",
    "\n",
    "training_data = add_total_bedrooms(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb471fee",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67ab932",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<br><br>\n",
    "\n",
    "<br/>\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "         \n",
    "## Question 5c\n",
    "\n",
    "Create a visualization that clearly and succinctly shows if there exists an association between  `Bedrooms` and `Log Sale Price`. A good visualization should satisfy the following requirements:\n",
    "- It should avoid overplotting.\n",
    "- It should have clearly labeled axes and a succinct title.\n",
    "- It should convey the strength of the correlation between `Sale Price` and the number of rooms: in other words, you should be able to look at the plot and describe the general relationship between `Log Sale Price` and `Bedrooms`\n",
    "\n",
    "**Hint 1**: A direct scatter plot of the `Sale Price` against the number of bedrooms for all of the households in our training data might risk overplotting. Try it out and see why causes overplotting. Then choose a different visualization method to avoid overplotting.\n",
    "\n",
    "**Hint 2**: Take a closer look at the variable `Bedrooms`. What variable type is `Bedrooms` and what are the possible values `Bedrooms` can take? What visualizations might be good at showing this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997b7440",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d75850",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>\n",
    "\n",
    "<br/>\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Question 6\n",
    "\n",
    "\n",
    "Now, let's take a look at the relationship between neighborhood and sale prices of the houses in our dataset.\n",
    "Notice that currently we don't have the actual names for the neighborhoods. Instead we will use a similar column `Neighborhood Code` (which is a numerical encoding of the actual neighborhoods by the Assessment office).\n",
    "\n",
    "\n",
    "## Question 6a\n",
    "\n",
    "Before creating any visualization, let's quickly inspect how many different neighborhoods we are dealing with.\n",
    "\n",
    "Assign the variable `num_neighborhoods` with the total number of unique neighborhoods in `training_data`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e887cd62",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "num_neighborhoods = ...\n",
    "num_neighborhoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95a92f5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494affbe",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "---\n",
    "## Question 6b\n",
    "\n",
    "If we try directly plotting the distribution of `Log Sale Price` for all of the households in each neighborhood using the `plot_categorical` function from the next cell, we would get the following visualization.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/q5p2_catplot.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86cfd7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Feel free to use the cell below this and run plot_categorical(training_data) if you want to see what this function outputs.\n",
    "\n",
    "def plot_categorical(neighborhoods):\n",
    "    fig, axs = plt.subplots(nrows=2)\n",
    "\n",
    "    sns.boxplot(\n",
    "        x='Neighborhood Code',\n",
    "        y='Log Sale Price',\n",
    "        data=neighborhoods,\n",
    "        ax=axs[0],\n",
    "    )\n",
    "\n",
    "    sns.countplot(\n",
    "        x='Neighborhood Code',\n",
    "        data=neighborhoods,\n",
    "        ax=axs[1],\n",
    "    )\n",
    "\n",
    "    # Draw median price\n",
    "    axs[0].axhline(\n",
    "        y=training_data['Log Sale Price'].median(), \n",
    "        color='red',\n",
    "        linestyle='dotted'\n",
    "    )\n",
    "\n",
    "    # Label the bars with counts\n",
    "    for patch in axs[1].patches:\n",
    "        x = patch.get_bbox().get_points()[:, 0]\n",
    "        y = patch.get_bbox().get_points()[1, 1]\n",
    "        axs[1].annotate(f'{int(y)}', (x.mean(), y), ha='center', va='bottom')\n",
    "\n",
    "    # Format x-axes\n",
    "    axs[1].set_xticklabels(axs[1].xaxis.get_majorticklabels(), rotation=90)\n",
    "    axs[0].xaxis.set_visible(False)\n",
    "\n",
    "    # Narrow the gap between the plots\n",
    "    plt.subplots_adjust(hspace=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac9c259-b475-47b1-a7fb-aeff25e5f40b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75cd5b5c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Oh no, looks like we have run into the problem of overplotting again! \n",
    "\n",
    "You might have noticed that the graph is overplotted because **there are actually quite a few neighborhoods in our dataset**! For the clarity of our visualization, we will have to zoom in again on a few of them. The reason for this is our visualization will become quite cluttered with a super dense x-axis.\n",
    "\n",
    "Assign the variable `top_15_neighborhood_codes` to a **list** of the 15 neighborhood codes that have the greatest number of property sales within them.\n",
    "\n",
    "\n",
    "Assign the variable `in_top_15_neighborhoods` to a copy of `training_data` that has been filtered to only contain rows corresponding to properties that are in one of the top 15 most populous neighborhoods. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ce3bb2",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "top_15_neighborhood_codes = ...\n",
    "\n",
    "in_top_15_neighborhoods = ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4831293",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378f1299",
   "metadata": {},
   "source": [
    "Let's create another of the distribution of sale price within in each neighborhood again, but this time with a narrower focus!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af07e8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_categorical(neighborhoods=in_top_15_neighborhoods)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46891895",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "---\n",
    "## Question 6c\n",
    "\n",
    "From the plot above, we can see that there is much less data available for some neighborhoods. For example, Neighborhood 81 has only around 33% of the number of datapoints as Neighborhood 30.\n",
    "\n",
    "One way we can deal with the lack of data from some neighborhoods is to create a new feature that bins neighborhoods together. We’ll categorize our neighborhoods in a crude way. In Question 6c, we’ll compute how “expensive” each neighborhood is by aggregating the `Log Sale Price`s for all properties in a particular neighborhood using a `metric`, such as the median. We’ll use this `metric` to find the top `n` most expensive neighborhoods. Then, in Question 6d, we’ll label these “expensive neighborhoods” and leave all other neighborhoods unmarked.\n",
    "\n",
    "Fill in `find_expensive_neighborhoods` to return a **list** of the neighborhood codes of the **top `n`** most expensive neighborhoods as measured by our choice of aggregating function, `metric`.\n",
    "\n",
    "For example, calling `find_expensive_neighborhoods(training_data, n=3, metric=np.median)` should return the 3 neighborhood codes with the highest **median `Log Sale Price`** computed across all properties in those neighborhood codes. \n",
    "\n",
    "Note that there the tests below just check that you have the correct format, NOT whether or not you have the correct answer.  That will be checked via a hidden test in Gradescope.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fce2479",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def find_expensive_neighborhoods(data, n=3, metric=np.median):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "      data (data frame): should contain at least a string-valued 'Neighborhood Code'\n",
    "        and a numeric 'Sale Price' column\n",
    "      n (int): the number of top values desired\n",
    "      metric (function): function used for aggregating the data in each neighborhood.\n",
    "        for example, np.median for median prices\n",
    "    \n",
    "    Output:\n",
    "      a list of the the neighborhood codes of the top n highest-priced neighborhoods as measured by the metric function\n",
    "    \"\"\"\n",
    "    neighborhoods = ...\n",
    "    \n",
    "    # This makes sure the final list contains the generic int type used in Python3, not specific ones used in numpy.\n",
    "    return [int(code) for code in neighborhoods]\n",
    "\n",
    "expensive_neighborhoods = find_expensive_neighborhoods(training_data, 3, np.median)\n",
    "expensive_neighborhoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1313e8",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09972ec1",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br><br>\n",
    "\n",
    "---\n",
    "## Question 6d\n",
    "\n",
    "We now have a list of neighborhoods we've deemed as higher-priced than others.  Let's use that information to write an additional function `add_expensive_neighborhood` that takes in a `DataFrame` of housing data (`data`) and a list of neighborhood codes considered to be expensive (`expensive_neighborhoods`). You can think of `expensive_neighborhoods` as being the output of the function `find_expensive_neighborhoods` from Question 6c. \n",
    "\n",
    "Using these inputs, `add_expensive_neighborhood` should add a column to `data` named `in_expensive_neighborhood` that takes on the **integer** value of 1 if a property is part of a neighborhood in `expensive_neighborhoods` and the integer value of 0 if it is not. This type of variable is known as an **indicator variable**.\n",
    "\n",
    "**Hint:** [`pd.Series.astype`](https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.Series.astype.html) may be useful for converting `True`/`False` values to integers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c354c1d",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def add_in_expensive_neighborhood(data, expensive_neighborhoods):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "      data (DataFrame): a DataFrame containing a 'Neighborhood Code' column with values\n",
    "        found in the codebook\n",
    "      expensive_neighborhoods (list of ints): ints should be the neighborhood codes of\n",
    "        neighborhoods pre-identified as expensive\n",
    "    Output:\n",
    "      DataFrame identical to the input with the addition of a binary\n",
    "      in_expensive_neighborhood column\n",
    "    \"\"\"\n",
    "    data['in_expensive_neighborhood'] = ...\n",
    "    return data\n",
    "\n",
    "expensive_neighborhoods = find_expensive_neighborhoods(training_data, 3, np.median)\n",
    "training_data = add_in_expensive_neighborhood(training_data, expensive_neighborhoods)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228554f7",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5d44aa",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br><br>\n",
    "\n",
    "In the following question, we will take a closer look at the `Roof Material` feature of the dataset and examine how we can incorporate categorical features into the linear models we will be developing in Part 2 of the project.\n",
    "\n",
    "---\n",
    "## Question 7a\n",
    "\n",
    "If we look at `codebook.txt` carefully, we can see that the Assessor's Office uses the following mapping for the numerical values in the `Roof Material` column.\n",
    "```\n",
    "Roof Material (Nominal): \n",
    "\n",
    "       1    Shingle/Asphalt\n",
    "       2    Tar&Gravel\n",
    "       3    Slate\n",
    "       4    Shake\n",
    "       5    Tile\n",
    "       6    Other\n",
    "```\n",
    "\n",
    "**NOTE** that Tar&Gravel has no spaces. That is, 'Tar&Gravel' $\\neq$ 'Tar & Gravel'\n",
    "\n",
    "Write a function `substitute_roof_material` that replaces each numerical value in `Roof Material` with a `string` of the corresponding roof material. Your function should return a new `DataFrame`, not modify the existing `DataFrame`.  If you modify the existing `DataFrame` by accident, you can load `training_data` again in `q2b`.\n",
    "\n",
    "**Hint**: the `DataFrame.replace` ([documentation](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.replace.html)) method may be useful here.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6539d437",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def substitute_roof_material(data):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "      data (DataFrame): a DataFrame containing a 'Roof Material' column.  Its values\n",
    "                         should be limited to those found in the codebook\n",
    "    Output:\n",
    "      new DataFrame identical to the input except with a refactored 'Roof Material' column\n",
    "    \"\"\"\n",
    "    ...\n",
    "    return data\n",
    "    \n",
    "training_data_mapped = substitute_roof_material(training_data) \n",
    "\n",
    "\n",
    "training_data_mapped[\"Roof Material\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f81fe3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q7a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220b428f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "## Question 7b\n",
    "\n",
    "#### One Hot Encoding \n",
    "\n",
    "Unfortunately, simply replacing the integers with the appropriate strings isn’t sufficient for using `Roof Material` in our model.  Since `Roof Material` is a categorical variable, we will have to one-hot-encode the data. For more information on why we want to use one-hot-encoding, refer to this [link](https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/).\n",
    "\n",
    "Complete the following function `ohe_roof_material` that returns a copy of the original dataframe with the new column one-hot-encoded on the roof material of the household. These new columns should have the form `Roof Material_MATERIAL`.  In this problem we only wish to construct the one-hot-encoding columns **without removing any columns**. Your function should return **a new DataFrame, not modify the existing DataFrame.**  \n",
    "\n",
    "**Note**: You should **avoid using `pd.get_dummies`** in your solution as it will remove your original column and is therefore not as reusable as your constructed data preprocessing pipeline. \n",
    "Instead should use `scikit-learn`’s [OneHotEncoder]((https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html))\n",
    "to perform the one-hot-encoding. `OneHotEncoder` will automatically generate column names of the form `Roof Material_MATERIAL`\n",
    "\n",
    "\n",
    "\n",
    "*Hint*: To get you started with this subpart, the next few boxes of code below initializes a `OneHotEncoding` pre-processing \"model\" from Scikit-learn and fits it on a simple dataset containing random names. Please play with this code before jumping into the roof material data if you are unsure how to approach the question using `OneHotEncoder`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0487e0e1",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Sample code to demonstrate One Hot Encoding\n",
    "\n",
    "d = {'Names': ['Anirudhan', 'Dominic', 'Rahul', 'Rahul', 'Anirudhan', 'Yike', 'Vasanth']}\n",
    "demo_df = pd.DataFrame(data=d)\n",
    "\n",
    "demo_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b497489b-2bd2-4613-a276-924954deeec7",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize the one hot encoder for sample demo\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc46065-a7af-42b8-a4ed-a3fb6c1c9bac",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "To \"learn\" the categories we fit the OneHotEncoder to the categorical data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b97ad7-e3d3-48b8-97e4-9bbac7b01776",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ohe.fit(demo_df[[\"Names\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f385ef0e-831a-4a10-bf66-ac4b8d12f74e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We can get the \"names\" of the new one-hot-encoding columns which reveal both the source columns and the categories within each column: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47581afa-f903-4a12-8db5-acb9301a90b4",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ohe.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0642ef-2732-4474-9534-24d145782cf2",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We can also construct the OneHotEncoding of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46efd16b-5758-41d0-a0ca-df0a833d96cd",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ohe.transform(demo_df[[\"Names\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b362b149-e846-434a-ae4d-da97f66a78b9",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Notice that the One-Hot-Encoding produces a sparse output matrix.  This is because most the entries are 0.    To view this matrix, we need to convert it to a numpy matrix using the following syntax:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf46055-cc5a-49f5-82d9-0c1faa7a83b3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "encoded_names = ohe.transform(demo_df[[\"Names\"]]).toarray()\n",
    "\n",
    "encoded_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9df8e3-e8fe-498d-b3d4-b0e60f4081f1",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We can then convert this to a DataFrame (to merge with our original demo_df DataFrame) as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f6d8e6-3c43-4d00-9364-99484d0fae06",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "\n",
    "encoded_demo_df = pd.DataFrame(encoded_names, columns=ohe.get_feature_names_out(), index = demo_df.index)\n",
    "\n",
    "encoded_demo_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781765dc-c8bb-4e90-a287-e0a471f952e6",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "#Now join back with original df:\n",
    "\n",
    "ohe_demo = demo_df.join(encoded_demo_df)\n",
    "\n",
    "ohe_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32da7ad-b735-4091-8a18-06a80e89d522",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Now it's your turn! \n",
    "Complete the following function `ohe_roof_material` that returns a copy of the original dataframe with the new column one-hot-encoded on the roof material of the household. These new columns should have the form `Roof Material_MATERIAL`.  In this problem we only wish to construct the one-hot-encoding columns **without removing any columns**. Your function should return **a new DataFrame, not modify the existing DataFrame.**  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50591a3e",
   "metadata": {
    "scrolled": true,
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def ohe_roof_material(data):\n",
    "    \"\"\"\n",
    "    One-hot-encodes roof material.  New columns are of the form \"Roof Material_MATERIAL\"\n",
    "    \"\"\"\n",
    "    ...\n",
    "\n",
    "training_data_ohe = ohe_roof_material(training_data_mapped)\n",
    "\n",
    "#This line of code will display only the one-hot-encoded columns in training_data_ohe that \n",
    "# have names that begin with “Roof Material_\" \n",
    "\n",
    "training_data_ohe.filter(regex='^Roof Material').head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ac5db1",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q7b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34e6e1e-464c-4a0e-9430-268030a5bd23",
   "metadata": {},
   "source": [
    "## Takeaways\n",
    "\n",
    "In this project, you have...\n",
    "- Peformed basic Exploratory Data Analysis (EDA)\n",
    "- Added new features to the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16be6aa5",
   "metadata": {},
   "source": [
    "## Congratulations! You have finished the Project Part 1!\n",
    "\n",
    "### What's next? \n",
    "\n",
    "In Project A2, you will focus on building a linear regression model to predict house prices. You are well-prepared to build such a model: you have considered what is in this dataset, what it can be used for, and engineered some features that should be useful for prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9a72a5-3477-4666-aa0b-455800d14e0c",
   "metadata": {},
   "source": [
    "If you discussed this assignment with any other students in the class (in a manner that is acceptable as described by the Collaboration policy above) please **include their names** here:\n",
    "\n",
    "**Collaborators**: *list collaborators here*\n",
    "\n",
    "If while completing this assignment you reference any websites other than those linked in this assignment or provided on Canvas please list those references here:\n",
    "\n",
    "**External references**:  *list any websites you referenced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0dd40d-f7c2-449e-b69c-e93e1744ca04",
   "metadata": {},
   "source": [
    "### Submission Instructions\n",
    "\n",
    "Before proceeding any further, **save this notebook.**\n",
    "\n",
    "After running the `grader.export()` cell provided below, **2 files will be created**: a zip file and pdf file.  You can download them using the links provided below OR by finding them in the same folder where this juptyer notebook resides in your JuptyerHub.\n",
    "\n",
    "To receive credit on this assignment, **you must submit BOTH of these files\n",
    "to their respective Gradescope portals:** \n",
    "\n",
    "* **Project Part 1 Autograded**: Submit the zip file that is output by the `grader.export()` cell below to the Project Part 1 Autograded assignment in Gradescope.\n",
    "\n",
    "* **Project Part 1 Manually Graded**: Submit your ProjPart1.PDF to the HW10 Manually Graded assignment in Gradescope.  **YOU MUST SELECT THE PAGES CORRESPONDING TO EACH QUESTION WHEN YOU UPLOAD TO GRADESCOPE.  IF NOT, YOU WILL LOSE POINTS**   Also, **check** that all of your plots **and** all lines of your code are showing up in your PDF before submitting.  If not, you will not receive credit for your plots/code.  \n",
    "\n",
    "\n",
    "**You are responsible for ensuring your submission follows our requirements. We will not be granting regrade requests nor extensions to submissions that don't follow instructions.** If you encounter any difficulties with submission, please don't hesitate to reach out to staff prior to the deadline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0f0c89-f294-421f-8794-4b88824d7e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import simple_latex_checker as slc\n",
    "\n",
    "nb = slc.Nb_checker()\n",
    "nb.run_check(\"ProjPart1.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f793c1",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**\n",
    "\n",
    "AFTER running the cell below, click on <a href='ProjPart1.pdf' download>this link to download the PDF </a> to upload to Gradescope.  There will be a separate link that appears after running the cell below with a link to download the zip file to upload to Gradescope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbf4f68",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(run_tests=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fea5a6",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q2ai": {
     "name": "q2ai",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert isinstance(q2ai, list) and len(q2ai) >= 3 and isinstance(q2ai[0], int)\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert get_hash(q2ai[0]) == 'c4ca4238a0b923820dcc509a6f75849b'\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert get_hash(q2ai[1]) == 'f899139df5e1059396431415e770c6dd'\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert get_hash(q2ai[2]) == 'd3d9446802a44259755d38e6d163e820'\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert get_hash(q2ai[3]) == '38b3eff8baf56627478ec76a704e9b52'\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2aii": {
     "name": "q2aii",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert get_hash(round(q2aii, 5)) == '8783110f5dbe08cb143cdb7b3e344db3'\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2b": {
     "name": "q2b",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert ('Sale Price' in training_data.columns) == True and ('Log Sale Price' in training_data.columns) == True\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert get_hash(round(training_data['Log Sale Price'].sum(), 3)) == '7781705cc447cc5af213156282d855fd'\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert training_data.shape[0] == 168931\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3a": {
     "name": "q3a",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert get_hash(round(training_data['Log Building Square Feet'].mean(), 3)) == 'a9af50facbc916992c1293e018cb4ec3'\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4a": {
     "name": "q4a",
     "points": [
      0,
      0,
      1,
      3
     ],
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert remove_outliers(training_data, 'Building Square Feet', upper=2000).shape != training_data.shape\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert np.isclose(sum(training_data['Building Square Feet']), 274753144.0, atol=0.1) == True\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert np.isclose(sum(remove_outliers(training_data, 'Sale Price', lower=121000, upper=1000000)['Building Square Feet']), 203454261.0, atol=0.1)\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert remove_outliers(training_data, 'Estimate (Building)', lower=0, upper=8187420).shape[0] == 167693\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4b": {
     "name": "q4b",
     "points": [
      0
     ],
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert set([q4bstatement]).issubset({False, True})\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4c": {
     "name": "q4c",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert get_hash(int(IQR)) == 'd2d1836b7375e85336296d909745a1ad'\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert q4c_training_data.shape[0] >= 120000 and q4c_training_data.shape[0] <= 160000\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert get_hash(int(q4c_training_data['Estimate (Land)'].max())) == '9117ad1cb16279cfce2a239e12e4c66c'\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert get_hash(int(q4c_training_data['Estimate (Land)'].min())) == 'cfcd208495d565ef66e7dff9f98764da'\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q5a": {
     "name": "q5a",
     "points": [
      0,
      2
     ],
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert type(q5a) == list\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert set([1, 2, 4, 6, 7, 8]) == set(q5a)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q5b": {
     "name": "q5b",
     "points": 4,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert not training_data['Bedrooms'].isnull().any()\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert get_hash(training_data['Bedrooms'].sum()) == '4ee51f0a44558acba5d037b211ae878a'\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert get_hash(training_data.loc[10000, 'Bedrooms']) == 'eccbc87e4b5ce2fe28308fd9f2a7baf3'\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert get_hash(sum(training_data.loc[:, 'Bedrooms'] < 5)) == '44ff361eb96953006ac4188e9c3ccc2a'\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q6a": {
     "name": "q6a",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert get_hash(num_neighborhoods) == 'bd686fd640be98efaae0091fa301e613'\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q6b": {
     "name": "q6b",
     "points": [
      0,
      0,
      0,
      2,
      1
     ],
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert get_hash(len(in_top_15_neighborhoods['Neighborhood Code'].unique())) == '9bf31c7ff062936a96d3c8bd1f8f2ff3'\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert isinstance(top_15_neighborhood_codes, list)\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert get_hash(in_top_15_neighborhoods['Neighborhood Code'].iloc[0]) == 'da4fb5c6e93e74d3df8527599fa62642'\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert get_hash(sum(top_15_neighborhood_codes)) == 'db576a7d2453575f29eab4bac787b919'\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert get_hash(in_top_15_neighborhoods['Neighborhood Code'].sum()) == 'a691bdb9bae6abe8b64fbe61b18b8726'\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q6c": {
     "name": "q6c",
     "points": [
      0,
      0
     ],
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert get_hash(len(find_expensive_neighborhoods(training_data, 5, np.median))) == 'e4da3b7fbbce2345d7772b0674a318d5'\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert isinstance(expensive_neighborhoods, list)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q6d": {
     "name": "q6d",
     "points": [
      0,
      2
     ],
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert sum(training_data.loc[:, 'in_expensive_neighborhood']) == 1290 and sum(training_data.loc[:, 'in_expensive_neighborhood'].isnull()) == 0\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert get_hash(sum(add_in_expensive_neighborhood(training_data, [44, 94, 93]).loc[:, 'in_expensive_neighborhood'])) == '70222949cc0db89ab32c9969754d4758'\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q7a": {
     "name": "q7a",
     "points": [
      0,
      2
     ],
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert set(training_data_mapped['Roof Material'].unique()) == set(['Shingle/Asphalt', 'Tar&Gravel', 'Other', 'Tile', 'Shake', 'Slate'])\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert training_data_mapped['Roof Material'].value_counts().values.tolist() == [160760, 3774, 1620, 1201, 841, 735]\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q7b": {
     "name": "q7b",
     "points": [
      1,
      1,
      2
     ],
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert training_data_ohe.shape == (168931, 72)\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> expected_ohe_cols = ['Roof Material_Other', 'Roof Material_Shake', 'Roof Material_Shingle/Asphalt', 'Roof Material_Slate', 'Roof Material_Tar&Gravel', 'Roof Material_Tile']\n>>> ohe_values = training_data_ohe[expected_ohe_cols].to_numpy()\n>>> assert ohe_values.sum() == training_data_ohe.shape[0]\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> expected_ohe_cols = set(['Roof Material_Other', 'Roof Material_Shake', 'Roof Material_Shingle/Asphalt', 'Roof Material_Slate', 'Roof Material_Tar&Gravel', 'Roof Material_Tile'])\n>>> assert expected_ohe_cols.issubset(set(training_data_ohe.columns)) == True\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
